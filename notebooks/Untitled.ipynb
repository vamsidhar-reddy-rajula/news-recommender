{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b5b4a3c-3fde-44ab-a44f-4dd3e3e3854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39ebc5b2-847c-4fb2-8702-d5066d95c5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [3, 4, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[3,4,0]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a967cd3-a265-47ff-9182-65b541c433cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [2, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69dee7f3-5cba-4221-b3bb-b64eb7f76da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31404e3f-f15e-4523-8c54-6178446ebd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import spacy\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443320a2-cc0b-465b-a3bb-5001fb9976b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = pd.read_csv(\"../data/interim/articles_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb773b8-8ad7-4b2d-97d8-57228b3955ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.article_published_on = df_articles.article_published_on.astype(np.datetime64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38f13b2a-d69e-4f20-80bd-b8d6a57106ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test = df_articles[df_articles.article_published_on<datetime(year=2021,day=20,month=8)],df_articles[df_articles.article_published_on>=datetime(year=2021,day=20,month=8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c144a66a-ef7a-43ec-9460-c7ba4fa76e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17655, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54827db0-5626-4e01-9b59-3d2681b1e293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(736, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98f963ac-1136-4ed7-8b82-74cc7481f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = joblib.load(\"../models/vectorizer_0830_0350\")\n",
    "model = joblib.load(\"../models/lda_model_0830_0350\")\n",
    "topic_vectors_train = joblib.load(\"../models/topic_vector_train_0830_0350\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66f640dd-3d8d-4a17-8262-3d81d102a33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why full FDA approval of Pfizer’s coronavirus vaccine matters\n",
      "\n",
      "\n",
      "2005     Swing voters, week 1: ‘Election is no slam-dunk’                              \n",
      "15454    US could see 200,000 Covid cases a day again: ‘Unvaccinated are sitting ducks’\n",
      "2493     Nation hits 70 percent vaccination goal amid surging Delta variant            \n",
      "10164    Who Are the Unvaccinated in America? There’s No One Answer.                   \n",
      "15690    ‘It’s too late’: US doctor says dying patients begging for Covid vaccine      \n",
      "2877     Biden administration sends more cash to hard-hit areas as Delta variant surges\n",
      "15821    99.2% of US Covid deaths in June were unvaccinated, says Fauci                \n",
      "3363     Delta variant said to be far more widespread than federal estimates           \n",
      "17497    Biden: US 'on track' to have enough vaccines for all adults by May            \n",
      "9051     Maryland confirms case of Covid-19 variant from South Africa                  \n",
      "Name: article_heading, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "test_indices = random.sample(range(df_test.shape[0]), 1)\n",
    "# test_indices=[0]\n",
    "test_lemmas = text_pipeline(df_test.iloc[test_indices].article_body)\n",
    "lemma_test_vectors = text_vectorizer(test_lemmas,vectorizer)\n",
    "topic_vectors_test = get_topic_vectors(lemma_test_vectors,model)\n",
    "similarity_scores = get_similar_articles(topic_vectors_test,topic_vectors_train)\n",
    "print_similar_articles(test_indices,similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "096a2298-dd09-4b48-b3e8-2d535953f324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why full FDA approval of Pfizer’s coronavirus vaccine matters\n",
      "\n",
      "\n",
      "4091     FDA tells J&J to scrap 60 million vaccine doses made at troubled plant                  \n",
      "4454     Biden admin probes risk of low-level contamination in vaccines made at troubled plant   \n",
      "11672    100 Million Vaccine Doses Held Up Over Contamination Concerns, Firm Reveals             \n",
      "4960     J&J vaccine production could restart in U.S. 'within days,' Emergent executive testifies\n",
      "6178     FDA orders J&J contractor Emergent to stop vaccine production during inspection         \n",
      "6544     Pfizer asks to OK Covid vaccine for younger teens                                       \n",
      "6816     Where will J&J produce millions of vaccines?                                            \n",
      "6842     FDA allows Moderna to put more coronavirus vaccine doses in each vial                   \n",
      "6854     Emergent admits to manufacturing issues with J&J vaccine                                \n",
      "7186     FDA authorizes J&J partner to help with vaccine production                              \n",
      "Name: article_heading, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test = \"Why full FDA approval of Pfizer’s coronavirus vaccine matters\"\n",
    "test_lemmas = text_pipeline(test)\n",
    "lemma_test_vectors = text_vectorizer(test_lemmas,vectorizer)\n",
    "topic_vectors_test = get_topic_vectors(lemma_test_vectors,model)\n",
    "similarity_scores = get_similar_articles(topic_vectors_test,topic_vectors_train)\n",
    "print_similar_articles(test_indices,similarity_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "134873c5-c037-4b41-93a7-9b1a0b62e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(X):\n",
    "    if isinstance(X, str):\n",
    "        X = pd.Series(X)\n",
    "    elif isinstance(X, (pd.Series, pd.DataFrame)):\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception(\n",
    "            f\"Input should either be in 'str' format or a 'series' or 'Dataframe' with a column of text. Received an object of type {type(X)}\"\n",
    "        )\n",
    "\n",
    "    # punctuations\n",
    "    removed_punctuation = X.apply(\n",
    "        lambda x: \"\".join([c for c in x if c not in punctuation])\n",
    "    )\n",
    "\n",
    "    # stop words\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    removed_stop_words = removed_punctuation.apply(\n",
    "        lambda x: \" \".join(\n",
    "            [word for word in word_tokenize(x) if word not in stop_words]\n",
    "        )\n",
    "    )\n",
    "    removed_stop_words = removed_stop_words.apply(lambda x: remove_stopwords(x))\n",
    "    all_stopwords_gensim = STOPWORDS.union(\n",
    "        set([\"the\", \"say\", \"said\", \"get\", \"it\", \"in\", \"like\", \"new\", \"year\"])\n",
    "    )\n",
    "    removed_stop_words = removed_stop_words.apply(\n",
    "        lambda x: \" \".join(\n",
    "            [word for word in word_tokenize(x) if word not in all_stopwords_gensim]\n",
    "        )\n",
    "    )\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    all_stopwords = sp.Defaults.stop_words\n",
    "    removed_stop_words = removed_stop_words.apply(\n",
    "        lambda x: \" \".join(\n",
    "            [word for word in word_tokenize(x) if word not in all_stopwords]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Stemming and Lematizing\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stem = removed_stop_words.apply(\n",
    "        lambda x: \" \".join([stemmer.stem(word) for word in word_tokenize(x)])\n",
    "    )\n",
    "    lemma = stem.apply(\n",
    "        lambda x: \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(x)])\n",
    "    )\n",
    "\n",
    "    return lemma   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0f902-c57d-4b21-9abe-b2e724b09091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce5c0075-0c94-49e1-8d9a-a77c47348def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_vectorizer(X,vectorizer,fit=False):\n",
    "    if fit:\n",
    "        return vectorizer.fit_transform(X)\n",
    "    else:\n",
    "        return vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0f002ca-a48c-4d6a-8873-a2e4746fb8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_vectors(X,model,fit=False):\n",
    "    if fit:\n",
    "        return model.fit_transform(X)\n",
    "    else:\n",
    "        return model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3e54fe0-2ce5-4dc4-8200-3801c2abf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_similar_articles(text_vectors, X, top_n_values=10):\n",
    "    \"\"\"\n",
    "    Evalute the cosine similarity between provided 'text_vectors' and trained X (articles trained and stored as a vecotr of topics).\n",
    "    Return dataframe with index as trained articles and columns as text_vector indices with values as similarity scores\n",
    "    \"\"\"\n",
    "    similarity_scores = cosine_similarity(X,text_vectors,dense_output=True)\n",
    "    return np.argsort(similarity_scores, axis=0)[::-1,:][:top_n_values,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bf0c99f-ade4-4e20-9a29-8a20efc36287",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 3)\n",
    "def print_similar_articles(test_indices,similarity_array):\n",
    "    for i in range(similarity_array.shape[1]):\n",
    "        indices = similarity_array[:,i]\n",
    "        print(\"\\n\")\n",
    "        print(df_test.iloc[test_indices[i]].article_heading)\n",
    "        print(\"\\n\")\n",
    "        print(df_train.iloc[indices].sort_values(['article_published_on'],ascending=False).article_heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7b41355-6e7d-460f-afa5-596ac3087516",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4213886f-bd55-40a0-b80c-ea2267dc6570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04b7e3d5-d64c-4412-b4c9-67808dafeaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.foxnews.com/politics/afghanistan-evacuation-us-f-18s-overwatch-flights-kabul'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[[1]].article_url.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c1d4d-022c-4bd7-9b6a-bd060e4ec6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Default Environment",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
