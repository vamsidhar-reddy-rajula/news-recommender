{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "957181bd-a27d-4455-b0c9-d5794023f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import contractions\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import spacy\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6602ede-34e7-4d12-98a4-8421543f0c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf9f6625-5154-46f1-afa4-bc970c4450ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = pd.read_csv(\"../data/interim/articles_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41778b8e-e5fd-4d18-9903-d81a83edca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    df_articles.article_published_on = df_articles.article_published_on.astype(np.datetime64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc88994-8f79-4e3e-9db6-45655d2b9796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test = df_articles[df_articles.article_published_on<datetime(year=2021,day=20,month=8)],df_articles[df_articles.article_published_on>=datetime(year=2021,day=20,month=8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc547ddf-07f3-4dd1-9f02-d1f68bde8e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40991ed9-7a26-46e1-84df-55d2c5afd1bd",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad522e12-8012-46b4-bfc7-32647e9c6442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8947934b-b526-4149-939a-4cf149f6e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_body = text_pipeline(df_test.article_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "415f34a3-f758-4ba4-89f6-3c5fcae2b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_heading = text_pipeline(df_test.article_heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ffb488c4-795d-4229-a3db-140423720633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_text = pd.DataFrame({'article_body':processed_body,'heading':processed_heading},index=df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7ea09faf-427a-44ef-a6f4-7d035adc8c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_text.to_csv(\"../data/processed/processed_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80ae4e77-9cd7-479f-adfd-8e4539971760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(X):\n",
    "    if isinstance(X, str):\n",
    "        X = pd.Series(X)\n",
    "    elif isinstance(X, (pd.Series, pd.DataFrame)):\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception(\n",
    "            f\"Input should either be in 'str' format or a 'series' or 'Dataframe' with a column of text. Received an object of type {type(X)}\"\n",
    "        )\n",
    "    \n",
    "    expanded_contractions = X.apply(lambda x:\n",
    "        contractions.fix(x)\n",
    "    )\n",
    "    \n",
    "    lower = expanded_contractions.str.lower()\n",
    "    \n",
    "    custom_preprocessor = lower.apply(lambda x: x.replace(\"-\",\" \").replace(\"'s\",\"\").replace(\"’s\",\"\").replace(\"–\",\"\"))\n",
    "\n",
    "    # punctuations\n",
    "    removed_punctuation = custom_preprocessor.apply(\n",
    "        lambda x: \"\".join([c for c in x if c not in punctuation])\n",
    "    )\n",
    "\n",
    "    # stop words\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    removed_stop_words = removed_punctuation.apply(\n",
    "        lambda x: \" \".join(\n",
    "            [word for word in word_tokenize(x) if word not in stop_words]\n",
    "        )\n",
    "    )\n",
    "    removed_stop_words = removed_stop_words.apply(lambda x: remove_stopwords(x))\n",
    "    all_stopwords_gensim = STOPWORDS.union(\n",
    "        set([\"the\", \"say\", \"said\", \"get\", \"it\", \"in\", \"like\", \"new\", \"year\"])\n",
    "    )\n",
    "    removed_stop_words = removed_stop_words.apply(\n",
    "        lambda x: \" \".join(\n",
    "            [word for word in word_tokenize(x) if word not in all_stopwords_gensim]\n",
    "        )\n",
    "    )\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    all_stopwords = sp.Defaults.stop_words\n",
    "    removed_stop_words = removed_stop_words.apply(\n",
    "        lambda x: \" \".join(\n",
    "            [word for word in word_tokenize(x) if word not in all_stopwords]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Stemming and Lematizing\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stem = removed_stop_words.apply(\n",
    "        lambda x: \" \".join([stemmer.stem(word) for word in word_tokenize(x)])\n",
    "    )\n",
    "    lemma = stem.apply(\n",
    "        lambda x: \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(x)])\n",
    "    )\n",
    "\n",
    "    return lemma   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f873e5-72cf-4636-a1ca-3ad7e1e502c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fbd92e4-f7cb-4777-b480-5417b9a9b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_vectorizer(X,vectorizer,fit=False):\n",
    "    if fit:\n",
    "        return vectorizer.fit_transform(X)\n",
    "    else:\n",
    "        return vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d8dce7f-665a-4aaf-96d4-bf86dca95e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_vectors(X,model,fit=False):\n",
    "    if fit:\n",
    "        return model.fit_transform(X)\n",
    "    else:\n",
    "        return model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51896ebb-556f-4aaf-91f5-8a3f97ff1efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_similar_articles(text_vectors, X, top_n_values=10):\n",
    "    \"\"\"\n",
    "    Evalute the cosine similarity between provided 'text_vectors' and trained X (articles trained and stored as a vecotr of topics).\n",
    "    Return dataframe with index as trained articles and columns as text_vector indices with values as similarity scores\n",
    "    \"\"\"\n",
    "    similarity_scores = cosine_similarity(X,text_vectors,dense_output=True)\n",
    "    return similarity_scores\n",
    "#     return np.argsort(similarity_scores, axis=0)[::-1,:][:top_n_values,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35152e50-5bb1-47ee-a1ee-5e8628424948",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 3)\n",
    "def print_similar_articles(test_indices,similarity_scores):\n",
    "    values = np.sort(similarity_scores, axis=0)[::-1,:][:5,:]\n",
    "    similarity_array = np.argsort(similarity_scores, axis=0)[::-1,:][:5,:]\n",
    "    for i in range(similarity_array.shape[1]):\n",
    "        indices = similarity_array[:,i]\n",
    "#         print(\"\\n\")\n",
    "#         print(df_test.iloc[test_indices[i]].article_heading)\n",
    "#         print(\"\\n\")\n",
    "#         print(df_train.iloc[indices].article_heading)\n",
    "#         print(\"\\n\")\n",
    "#         print(values[:,i])\n",
    "        return df_test.iloc[test_indices[i]].article_heading,pd.DataFrame({'article_heading':df_train.iloc[indices].article_heading,'Similarity score':values[:,i],'article_date':df_train.iloc[indices].article_published_on})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "448f59df-4ca0-433c-a413-161a6a31293f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "194aa3a4-1941-4857-b3f6-7a9027e1dfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_models_to_memory(n_components):\n",
    "    base_path = \"D:\\\\Models\\\\news recommender\\\\\"\n",
    "    if n_components == 300:\n",
    "        topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_1513_300\")\n",
    "        vectorizer = joblib.load(base_path + \"lda_model_0830_1513_300\")\n",
    "        model = joblib.load(base_path + \"topic_vector_train_0830_1513_300\")\n",
    "    elif n_components == 240:\n",
    "        topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_1406_240\")\n",
    "        vectorizer = joblib.load(base_path + \"lda_model_0830_1406_240\")\n",
    "        model = joblib.load(base_path + \"topic_vector_train_0830_1406_240\")\n",
    "    elif n_components == 180:\n",
    "        topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_1304_180\")\n",
    "        vectorizer = joblib.load(base_path + \"lda_model_0830_1304_180\")\n",
    "        model = joblib.load(base_path + \"topic_vector_train_0830_1304_180\")\n",
    "    elif n_components == 150:\n",
    "        topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_1205_150\")\n",
    "        vectorizer = joblib.load(base_path + \"lda_model_0830_1205_150\")\n",
    "        model = joblib.load(base_path + \"topic_vector_train_0830_1205_150\")\n",
    "    elif n_components == 120:\n",
    "        topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_1109_120\")\n",
    "        vectorizer = joblib.load(base_path + \"lda_model_0830_1109_120\")\n",
    "        model = joblib.load(base_path + \"topic_vector_train_0830_1109_120\")\n",
    "    elif n_components == 90:\n",
    "        topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_1015_90\")\n",
    "        vectorizer = joblib.load(base_path + \"lda_model_0830_1015_90\")\n",
    "        model = joblib.load(base_path + \"topic_vector_train_0830_1015_90\")\n",
    "    elif n_components == 60:\n",
    "        topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_0925_60\")\n",
    "        vectorizer = joblib.load(base_path + \"lda_model_0830_0925_60\")\n",
    "        model = joblib.load(base_path + \"topic_vector_train_0830_0925_60\")\n",
    "    elif n_components == 30:\n",
    "        topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_0838_30\")\n",
    "        vectorizer = joblib.load(base_path + \"lda_model_0830_0838_30\")\n",
    "        model = joblib.load(base_path + \"topic_vector_train_0830_0838_30\")\n",
    "    saved_models[n_components][\"topic_vectors_train\"]=topic_vectors_train\n",
    "    saved_models[n_components][\"vectorizer\"]=vectorizer\n",
    "    saved_models[n_components][\"model\"]=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "deaee883-cbd2-485b-a1e2-c1777066315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models = {300:{},240:{},180:{},150:{},120:{},90:{},60:{},30:{}}\n",
    "for component in components_saved:\n",
    "    load_saved_models_to_memory(component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79eb0321-c0c7-4a84-b2a9-4bae0197b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_saved_models(n_components):\n",
    "#     base_path = \"D:\\\\Models\\\\news recommender\\\\\"\n",
    "#     if n_components == 300:\n",
    "#         topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_1513_300\")\n",
    "#         vectorizer = joblib.load(base_path + \"lda_model_0830_1513_300\")\n",
    "#         model = joblib.load(base_path + \"topic_vector_train_0830_1513_300\")\n",
    "#     elif n_components == 240:\n",
    "#         topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_1406_240\")\n",
    "#         vectorizer = joblib.load(base_path + \"lda_model_0830_1406_240\")\n",
    "#         model = joblib.load(base_path + \"topic_vector_train_0830_1406_240\")\n",
    "#     elif n_components == 180:\n",
    "#         topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_1304_180\")\n",
    "#         vectorizer = joblib.load(base_path + \"lda_model_0830_1304_180\")\n",
    "#         model = joblib.load(base_path + \"topic_vector_train_0830_1304_180\")\n",
    "#     elif n_components == 150:\n",
    "#         topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_1205_150\")\n",
    "#         vectorizer = joblib.load(base_path + \"lda_model_0830_1205_150\")\n",
    "#         model = joblib.load(base_path + \"topic_vector_train_0830_1205_150\")\n",
    "#     elif n_components == 120:\n",
    "#         topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_1109_120\")\n",
    "#         vectorizer = joblib.load(base_path + \"lda_model_0830_1109_120\")\n",
    "#         model = joblib.load(base_path + \"topic_vector_train_0830_1109_120\")\n",
    "#     elif n_components == 90:\n",
    "#         topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_1015_90\")\n",
    "#         vectorizer = joblib.load(base_path + \"lda_model_0830_1015_90\")\n",
    "#         model = joblib.load(base_path + \"topic_vector_train_0830_1015_90\")\n",
    "#     elif n_components == 60:\n",
    "#         topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_0925_60\")\n",
    "#         vectorizer = joblib.load(base_path + \"lda_model_0830_0925_60\")\n",
    "#         model = joblib.load(base_path + \"topic_vector_train_0830_0925_60\")\n",
    "#     elif n_components == 30:\n",
    "#         topic_vectors_train = joblib.load(base_path + \"vectorizer_0830_0838_30\")\n",
    "#         vectorizer = joblib.load(base_path + \"lda_model_0830_0838_30\")\n",
    "#         model = joblib.load(base_path + \"topic_vector_train_0830_0838_30\")\n",
    "#     return topic_vectors_train, vectorizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dfa4513-205a-4a95-b96c-cd2c113f7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_saved = [300,240,180,150,120,90,60,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd679d3-cd7c-44c6-88b9-cc70a829614c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de57baa1-4680-4e03-ab46-9a2348e73d5e",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19c4e60c-8c2f-46b0-9294-a0839d648cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_articles(test_indices,similarity_scores, top_n_values = 5):\n",
    "    values = np.sort(similarity_scores, axis=0)[::-1,:][:top_n_values,:]\n",
    "    similarity_array = np.argsort(similarity_scores, axis=0)[::-1,:][:top_n_values,:]\n",
    "    for i in range(similarity_array.shape[1]):\n",
    "        indices = similarity_array[:,i]\n",
    "    return df_test.iloc[test_indices[i]].article_heading,pd.DataFrame({'article_heading':df_train.iloc[indices].article_heading,'Similarity score':values[:,i],'article_date':df_train.iloc[indices].article_published_on})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d3bcc4b-3479-4925-9dbf-6f91b864b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_article_importance_day_wise(growth=1000):\n",
    "    diff_from_max_date = (df_train.article_published_on-(df_train.article_published_on.max())).dt.days\n",
    "    return np.exp(diff_from_max_date/growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3768203-d75e-416a-90f8-d7b8734070f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_score(text_vectors, X, factor=None):\n",
    "    \"\"\"\n",
    "    Evalute the cosine similarity between provided 'text_vectors' and trained X (articles trained and stored as a vecotr of topics).\n",
    "    Return dataframe with index as trained articles and columns as text_vector indices with values as similarity scores\n",
    "    \"\"\"\n",
    "    similarity_scores = cosine_similarity(X,text_vectors,dense_output=True)\n",
    "    return similarity_scores*factor\n",
    "#     return np.argsort(similarity_scores, axis=0)[::-1,:][:top_n_values,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1dbc0f39-c5bc-41b7-aa3d-3400e8a9c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news_article(component,include_headings=False,heading_weightage=0.6,test_indices=None,factor=np.ones((df_train.shape[0])).reshape(-1,1)):\n",
    "    topic_vectors_train,vectorizer,model = saved_models[component][\"topic_vectors_train\"],saved_models[component][\"vectorizer\"],saved_models[component][\"model\"]\n",
    "    test_lemmas = text_pipeline(df_test.iloc[test_indices].article_body)\n",
    "    lemma_test_vectors = text_vectorizer(test_lemmas,vectorizer)\n",
    "    topic_vectors_test = get_topic_vectors(lemma_test_vectors,model)\n",
    "    similarity_scores = get_similarity_score(topic_vectors_test,topic_vectors_train,factor=factor)\n",
    "    heading, result = get_similar_articles(test_indices,similarity_scores)\n",
    "    if include_headings:\n",
    "        test_lemmas = text_pipeline(df_test.iloc[test_indices].article_heading)\n",
    "        lemma_test_vectors = text_vectorizer(test_lemmas,vectorizer)\n",
    "        topic_vectors_test = get_topic_vectors(lemma_test_vectors,model)\n",
    "        similarity_scores = heading_weightage*(get_similarity_score(topic_vectors_test,topic_vectors_train,factor=factor))+(1-heading_weightage)*similarity_scores\n",
    "        _, result = get_similar_articles(test_indices,similarity_scores)\n",
    "    return heading, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c680cb43-9b34-455c-a3c8-1d802637e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del saved_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82734c8c-8b96-4bc4-ab26-477ffc7b1bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_body', 'heading'], dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text = pd.read_csv(r\"../data/processed/processed_test_data.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0db7daa3-9b0f-46ca-b928-20c7e4c29577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_similarity_scores(components,include_headings=False,heading_weightage=0.6,test_indices=None,factor=np.ones((df_train.shape[0])).reshape(-1,1)):\n",
    "    component_similarity_scores = []\n",
    "    for component in components:\n",
    "        topic_vectors_train,vectorizer,model = saved_models[component][\"topic_vectors_train\"],saved_models[component][\"vectorizer\"],saved_models[component][\"model\"]\n",
    "#         test_lemmas = text_pipeline(df_test.iloc[test_indices].article_body)\n",
    "        test_lemmas = processed_text.iloc[test_indices].article_body\n",
    "        lemma_test_vectors = text_vectorizer(test_lemmas,vectorizer)\n",
    "        topic_vectors_test = get_topic_vectors(lemma_test_vectors,model)\n",
    "        similarity_scores = get_similarity_score(topic_vectors_test,topic_vectors_train,factor=factor)\n",
    "#         heading, result = get_similar_articles(test_indices,similarity_scores)\n",
    "        if include_headings:\n",
    "#             test_lemmas = text_pipeline(df_test.iloc[test_indices].article_heading)\n",
    "            test_lemmas = processed_text.iloc[test_indices].heading\n",
    "            lemma_test_vectors = text_vectorizer(test_lemmas,vectorizer)\n",
    "            topic_vectors_test = get_topic_vectors(lemma_test_vectors,model)\n",
    "            similarity_scores = heading_weightage*(get_similarity_score(topic_vectors_test,topic_vectors_train,factor=factor))+(1-heading_weightage)*similarity_scores\n",
    "#             _, result = get_similar_articles(test_indices,similarity_scores)\n",
    "        component_similarity_scores.append(similarity_scores)\n",
    "    return component_similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f587cf8f-f13a-4624-b1a5-cdd57a3a19ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502 ms ± 52.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "import random\n",
    "test_indices = random.sample(range(df_test.shape[0]), 1)\n",
    "# test_indices = [0]\n",
    "factor=np.ones((df_train.shape[0])).reshape(-1,1)\n",
    "weights = [\n",
    "        0.0392685,\n",
    "        0.09838475,\n",
    "        0.04760199,\n",
    "        0.05147573,\n",
    "        0.04382252,\n",
    "        0.0741635,\n",
    "        0.04844888,\n",
    "        0.59683412,\n",
    "    ]\n",
    "# weights = np.random.dirichlet(np.ones(8),size=1).reshape(8,)\n",
    "# factor = get_article_importance_day_wise(growth=1000).values.reshape(-1,1)\n",
    "component_similarity_scores = ensemble_similarity_scores(components=components_saved,include_headings=True,heading_weightage=0.75,test_indices=test_indices,factor=factor)\n",
    "# similarity_scores = np.average(np.array(component_similarity_scores),axis=0,weights=weights)\n",
    "# heading, result = get_similar_articles(test_indices,similarity_scores,top_n_values=10)\n",
    "# print(heading)\n",
    "# display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4df22618-0f32-450d-90d3-993b98f53524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-07 s\n",
       "\n",
       "Total time: 8.75041 s\n",
       "File: C:\\Users\\rvams\\AppData\\Local\\Temp/ipykernel_23776/2654383235.py\n",
       "Function: ensemble_similarity_scores at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def ensemble_similarity_scores(components,include_headings=False,heading_weightage=0.6,test_indices=None,factor=np.ones((df_train.shape[0])).reshape(-1,1)):\n",
       "     2         1         27.0     27.0      0.0      component_similarity_scores = []\n",
       "     3         9         95.0     10.6      0.0      for component in components:\n",
       "     4         8        202.0     25.2      0.0          topic_vectors_train,vectorizer,model = saved_models[component][\"topic_vectors_train\"],saved_models[component][\"vectorizer\"],saved_models[component][\"model\"]\n",
       "     5         8   40576782.0 5072097.8     46.4          test_lemmas = text_pipeline(df_test.iloc[test_indices].article_body)\n",
       "     6         8     154899.0  19362.4      0.2          lemma_test_vectors = text_vectorizer(test_lemmas,vectorizer)\n",
       "     7         8     962449.0 120306.1      1.1          topic_vectors_test = get_topic_vectors(lemma_test_vectors,model)\n",
       "     8         8    1423588.0 177948.5      1.6          similarity_scores = get_similarity_score(topic_vectors_test,topic_vectors_train,factor=factor)\n",
       "     9                                           #         heading, result = get_similar_articles(test_indices,similarity_scores)\n",
       "    10         8        177.0     22.1      0.0          if include_headings:\n",
       "    11         8   42828305.0 5353538.1     48.9              test_lemmas = text_pipeline(df_test.iloc[test_indices].article_heading)\n",
       "    12         8      22879.0   2859.9      0.0              lemma_test_vectors = text_vectorizer(test_lemmas,vectorizer)\n",
       "    13         8      96022.0  12002.8      0.1              topic_vectors_test = get_topic_vectors(lemma_test_vectors,model)\n",
       "    14         8    1438421.0 179802.6      1.6              similarity_scores = heading_weightage*(get_similarity_score(topic_vectors_test,topic_vectors_train,factor=factor))+(1-heading_weightage)*similarity_scores\n",
       "    15                                           #             _, result = get_similar_articles(test_indices,similarity_scores)\n",
       "    16         8        273.0     34.1      0.0          component_similarity_scores.append(similarity_scores)\n",
       "    17         1          4.0      4.0      0.0      return component_similarity_scores"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "test_indices = random.sample(range(df_test.shape[0]), 1)\n",
    "# test_indices = [0]\n",
    "factor=np.ones((df_train.shape[0])).reshape(-1,1)\n",
    "weights = [\n",
    "        0.0392685,\n",
    "        0.09838475,\n",
    "        0.04760199,\n",
    "        0.05147573,\n",
    "        0.04382252,\n",
    "        0.0741635,\n",
    "        0.04844888,\n",
    "        0.59683412,\n",
    "    ]\n",
    "%lprun -f ensemble_similarity_scores ensemble_similarity_scores(components=components_saved,include_headings=True,heading_weightage=0.75,test_indices=test_indices,factor=factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4c6d8-6779-4ab9-8b89-da0915dab5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Default Environment",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
